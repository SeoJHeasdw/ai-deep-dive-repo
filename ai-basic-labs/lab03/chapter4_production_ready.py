"""
RAG (Retrieval-Augmented Generation) í”„ë¡œë•ì…˜ ì¤€ë¹„
Chapter 4: í”„ë¡œë•ì…˜ ì¤€ë¹„ - RAG í‰ê°€, Citation, Streaming

ì‹¤ìŠµ í•­ëª©:
8. RAG í‰ê°€ - Faithfulness, Relevancy ì¸¡ì •
9. Citation ì¶œì²˜ í‘œê¸° - ë‹µë³€ì— ì¶œì²˜ ë‹¬ê¸°
10. Streaming ì‘ë‹µ - ì‹¤ì‹œê°„ í† í° ì¶œë ¥

í•™ìŠµ ëª©í‘œ:
- RAG ì‹œìŠ¤í…œ í’ˆì§ˆ ì¸¡ì • ë°©ë²• ì´í•´
- ì¶œì²˜ í‘œê¸°ë¥¼ í†µí•œ ì‹ ë¢°ì„± í™•ë³´
- Streamingìœ¼ë¡œ UX ê°œì„ 
- ì‹¤ì„œë¹„ìŠ¤ ë°°í¬ ì¤€ë¹„
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import chromadb
from openai import OpenAI
from dotenv import load_dotenv
import tiktoken
import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ .env íŒŒì¼ ë¡œë“œ
project_root = Path(__file__).parent.parent
load_dotenv(dotenv_path=project_root / '.env')

# ê³µí†µ ìœ í‹¸ë¦¬í‹° importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(project_root))
from utils import (
    print_section_header,
    print_subsection,
    print_key_points,
    get_openai_client,
    interpret_l2_distance,
    l2_distance_to_similarity
)

# ê³µí†µ ë°ì´í„° ì„í¬íŠ¸
from shared_data import SAMPLE_TEXT, MIN_TEXT_LENGTH, get_sample_or_document_text


# ============================================================================
# RAG í‰ê°€
# ============================================================================

class RAGEvaluator:
    """RAG ì‹œìŠ¤í…œ í’ˆì§ˆ í‰ê°€"""
    
    def __init__(self):
        self.client = get_openai_client()
        self.model = "gpt-4o-mini"
    
    def evaluate_faithfulness(self, answer: str, context: str) -> Dict[str, Any]:
        """
        Faithfulness í‰ê°€: ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì— ì¶©ì‹¤í•œì§€
        (í™˜ê° ê°ì§€)
        """
        prompt = f"""ë‹¹ì‹ ì€ RAG ì‹œìŠ¤í…œ í‰ê°€ìì…ë‹ˆë‹¤.
ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ëŠ”ì§€ í‰ê°€í•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ë‹µë³€:
{answer}

í‰ê°€ ê¸°ì¤€:
- ë‹µë³€ì˜ ëª¨ë“  ì •ë³´ê°€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ìœ ë˜í•˜ëŠ”ê°€?
- ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ë¥¼ ì¶”ê°€í•˜ì§€ ì•Šì•˜ëŠ”ê°€?
- ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ë¥¼ ì™œê³¡í•˜ì§€ ì•Šì•˜ëŠ”ê°€?

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "score": 0.0~1.0 ì‚¬ì´ì˜ ì ìˆ˜,
    "faithful_parts": ["ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•œ ë¶€ë¶„ë“¤"],
    "hallucinated_parts": ["ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ë“¤"],
    "explanation": "í‰ê°€ ì„¤ëª…"
}}"""
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1
        )
        
        import json
        try:
            result = response.choices[0].message.content.strip()
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            if '{' in result:
                json_str = result[result.find('{'):result.rfind('}')+1]
                return json.loads(json_str)
        except:
            pass
        
        return {"score": 0.5, "explanation": "í‰ê°€ ì‹¤íŒ¨"}
    
    def evaluate_relevancy(self, answer: str, question: str) -> Dict[str, Any]:
        """
        Answer Relevancy í‰ê°€: ë‹µë³€ì´ ì§ˆë¬¸ì— ì ì ˆí•œì§€
        """
        prompt = f"""ë‹¹ì‹ ì€ RAG ì‹œìŠ¤í…œ í‰ê°€ìì…ë‹ˆë‹¤.
ë‹µë³€ì´ ì§ˆë¬¸ì— ì ì ˆí•˜ê²Œ ëŒ€ë‹µí•˜ëŠ”ì§€ í‰ê°€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸:
{question}

ë‹µë³€:
{answer}

í‰ê°€ ê¸°ì¤€:
- ë‹µë³€ì´ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ë‹µí•˜ëŠ”ê°€?
- ë¶ˆí•„ìš”í•œ ì •ë³´ê°€ ë§ì§€ ì•Šì€ê°€?
- ì§ˆë¬¸ì˜ í•µì‹¬ì„ íŒŒì•…í–ˆëŠ”ê°€?

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "score": 0.0~1.0 ì‚¬ì´ì˜ ì ìˆ˜,
    "addresses_question": true/false,
    "missing_aspects": ["ë‹µë³€ì—ì„œ ëˆ„ë½ëœ ë¶€ë¶„"],
    "explanation": "í‰ê°€ ì„¤ëª…"
}}"""
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1
        )
        
        import json
        try:
            result = response.choices[0].message.content.strip()
            if '{' in result:
                json_str = result[result.find('{'):result.rfind('}')+1]
                return json.loads(json_str)
        except:
            pass
        
        return {"score": 0.5, "explanation": "í‰ê°€ ì‹¤íŒ¨"}
    
    def evaluate_context_precision(self, contexts: List[str], question: str) -> Dict[str, Any]:
        """
        Context Precision í‰ê°€: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì–¼ë§ˆë‚˜ ì •í™•í•œì§€
        """
        contexts_text = "\n\n".join([f"[ë¬¸ì„œ {i+1}] {c}" for i, c in enumerate(contexts)])
        
        prompt = f"""ë‹¹ì‹ ì€ RAG ì‹œìŠ¤í…œ í‰ê°€ìì…ë‹ˆë‹¤.
ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì´ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ”ì§€ í‰ê°€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸:
{question}

ê²€ìƒ‰ëœ ë¬¸ì„œë“¤:
{contexts_text}

ê° ë¬¸ì„œê°€ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë° ìœ ìš©í•œì§€ í‰ê°€í•˜ê³ ,
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "overall_score": 0.0~1.0 ì‚¬ì´ì˜ ì „ì²´ ì ìˆ˜,
    "document_scores": [ê° ë¬¸ì„œì˜ ê´€ë ¨ì„± ì ìˆ˜ ë¦¬ìŠ¤íŠ¸],
    "useful_documents": [ìœ ìš©í•œ ë¬¸ì„œ ë²ˆí˜¸ë“¤],
    "explanation": "í‰ê°€ ì„¤ëª…"
}}"""
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1
        )
        
        import json
        try:
            result = response.choices[0].message.content.strip()
            if '{' in result:
                json_str = result[result.find('{'):result.rfind('}')+1]
                return json.loads(json_str)
        except:
            pass
        
        return {"overall_score": 0.5, "explanation": "í‰ê°€ ì‹¤íŒ¨"}


# ============================================================================
# Citation ì¶œì²˜ í‘œê¸°
# ============================================================================

class CitationRAG:
    """ì¶œì²˜ë¥¼ í‘œê¸°í•˜ëŠ” RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.client = get_openai_client()
        self.model = "gpt-4o-mini"
    
    def generate_with_citation(self, question: str, contexts: List[Dict]) -> Dict:
        """ì¶œì²˜ë¥¼ í¬í•¨í•œ ë‹µë³€ ìƒì„±"""
        # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ… (ë²ˆí˜¸ ë¶€ì—¬)
        formatted_contexts = []
        for i, ctx in enumerate(contexts, 1):
            formatted_contexts.append(f"[{i}] {ctx['content']}")
        
        context_text = "\n\n".join(formatted_contexts)
        
        prompt = f"""ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
ë‹µë³€ ì‹œ ë°˜ë“œì‹œ ì¶œì²˜ë¥¼ [1], [2] í˜•ì‹ìœ¼ë¡œ í‘œê¸°í•´ì£¼ì„¸ìš”.

ì°¸ê³  ë¬¸ì„œ:
{context_text}

ì§ˆë¬¸: {question}

ë‹µë³€ (ì¶œì²˜ ë²ˆí˜¸ í¬í•¨):"""
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì¶œì²˜ë¥¼ ëª…í™•íˆ í‘œê¸°í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ëª¨ë“  ì •ë³´ì— ì¶œì²˜ ë²ˆí˜¸ [1], [2] ë“±ì„ í‘œê¸°í•´ì£¼ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3
        )
        
        answer = response.choices[0].message.content
        
        # ì‚¬ìš©ëœ ì¶œì²˜ ì¶”ì¶œ
        import re
        used_citations = list(set(re.findall(r'\[(\d+)\]', answer)))
        used_citations.sort(key=int)
        
        return {
            'answer': answer,
            'used_citations': used_citations,
            'sources': contexts
        }


# ============================================================================
# ë°ëª¨ í•¨ìˆ˜ë“¤
# ============================================================================

def demo_rag_evaluation():
    """ì‹¤ìŠµ 8: RAG í‰ê°€ - Faithfulness, Relevancy ì¸¡ì •"""
    print("\n" + "="*80)
    print("[8] ì‹¤ìŠµ 8: RAG í‰ê°€ - Faithfulness, Relevancy ì¸¡ì •")
    print("="*80)
    print("ëª©í‘œ: RAG ì‹œìŠ¤í…œì˜ í’ˆì§ˆì„ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •")
    print("í•µì‹¬: í™˜ê° ê°ì§€, ë‹µë³€ ì ì ˆì„±, ê²€ìƒ‰ ì •í™•ë„ í‰ê°€")
    
    # RAG í‰ê°€ì˜ ì¤‘ìš”ì„±
    print_section_header("RAG í‰ê°€ê°€ ì¤‘ìš”í•œ ì´ìœ ", "[INFO]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [!] ë¬¸ì œ: RAG ì‹œìŠ¤í…œì˜ í’ˆì§ˆì„ ì–´ë–»ê²Œ ì¸¡ì •í• ê¹Œ?         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                         â”‚
  â”‚  ë‹¨ìˆœíˆ "ë‹µë³€ì´ ì¢‹ì•„ ë³´ì¸ë‹¤"ëŠ” ì£¼ê´€ì  í‰ê°€ë¡œëŠ” ë¶€ì¡±!    â”‚
  â”‚                                                         â”‚
  â”‚  í•„ìš”í•œ ê²ƒ:                                             â”‚
  â”‚  * ì •ëŸ‰ì  ë©”íŠ¸ë¦­ (ìˆ«ìë¡œ í‘œí˜„)                          â”‚
  â”‚  * ìë™í™”ëœ í‰ê°€ (ëŒ€ëŸ‰ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥)                     â”‚
  â”‚  * ì¬í˜„ ê°€ëŠ¥í•œ í‰ê°€ (A/B í…ŒìŠ¤íŠ¸)                        â”‚
  â”‚                                                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  [RAGAS] RAG í‰ê°€ í”„ë ˆì„ì›Œí¬ì˜ í•µì‹¬ ë©”íŠ¸ë¦­              â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                         â”‚
  â”‚  1. Faithfulness (ì¶©ì‹¤ë„)                               â”‚
  â”‚     * ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì—ë§Œ ê¸°ë°˜í•˜ëŠ”ê°€?                   â”‚
  â”‚     * í™˜ê°(hallucination) ê°ì§€                          â”‚
  â”‚     * 0~1 ì ìˆ˜ (1ì´ ìµœê³ )                               â”‚
  â”‚                                                         â”‚
  â”‚  2. Answer Relevancy (ë‹µë³€ ì ì ˆì„±)                      â”‚
  â”‚     * ë‹µë³€ì´ ì§ˆë¬¸ì— ì ì ˆíˆ ëŒ€ë‹µí•˜ëŠ”ê°€?                  â”‚
  â”‚     * ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ì˜ë¯¸ì  ì—°ê´€ì„±                       â”‚
  â”‚     * 0~1 ì ìˆ˜ (1ì´ ìµœê³ )                               â”‚
  â”‚                                                         â”‚
  â”‚  3. Context Precision (ì»¨í…ìŠ¤íŠ¸ ì •ë°€ë„)                 â”‚
  â”‚     * ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì§ˆë¬¸ì— ê´€ë ¨ ìˆëŠ”ê°€?                 â”‚
  â”‚     * ë¶ˆí•„ìš”í•œ ë¬¸ì„œê°€ ì„ì—¬ ìˆì§€ ì•Šì€ê°€?                 â”‚
  â”‚     * 0~1 ì ìˆ˜ (1ì´ ìµœê³ )                               â”‚
  â”‚                                                         â”‚
  â”‚  4. Context Recall (ì»¨í…ìŠ¤íŠ¸ ì¬í˜„ìœ¨)                    â”‚
  â”‚     * í•„ìš”í•œ ì •ë³´ê°€ ëª¨ë‘ ê²€ìƒ‰ë˜ì—ˆëŠ”ê°€?                  â”‚
  â”‚     * Ground Truth ì •ë‹µì´ í•„ìš”í•¨                        â”‚
  â”‚     * 0~1 ì ìˆ˜ (1ì´ ìµœê³ )                               â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    if not os.getenv("OPENAI_API_KEY"):
        print("\n[!] OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
        return
    
    evaluator = RAGEvaluator()
    
    # í‰ê°€ ì˜ˆì‹œ ë°ì´í„°
    question = "RAG ì‹œìŠ¤í…œì—ì„œ ì²­í‚¹(chunking)ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"
    
    context = """ì²­í‚¹(Chunking)ì€ ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. 
RAG ì‹œìŠ¤í…œì—ì„œ ì²­í‚¹ì´ ì¤‘ìš”í•œ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
1. LLMì˜ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì œí•œ (ì˜ˆ: 4K~128K í† í°)
2. ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ (ì‘ì€ ë‹¨ìœ„ë¡œ ê²€ìƒ‰)
3. ê´€ë ¨ ì •ë³´ë§Œ ì„ ë³„í•˜ì—¬ ì „ë‹¬
ì¼ë°˜ì ìœ¼ë¡œ 500~1000ì ë‹¨ìœ„ë¡œ ì²­í‚¹í•˜ë©°, ì˜¤ë²„ë©ì„ ë‘ì–´ ë¬¸ë§¥ì„ ìœ ì§€í•©ë‹ˆë‹¤."""
    
    good_answer = """ì²­í‚¹ì€ ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. 
RAGì—ì„œ ì²­í‚¹ì´ ì¤‘ìš”í•œ ì´ìœ ëŠ” LLMì˜ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì œí•œ, ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ, 
ê´€ë ¨ ì •ë³´ ì„ ë³„ ë“±ì´ ìˆìŠµë‹ˆë‹¤. ë³´í†µ 500~1000ì ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³  ì˜¤ë²„ë©ì„ ë‘¡ë‹ˆë‹¤."""
    
    bad_answer = """ì²­í‚¹ì€ ë¬¸ì„œë¥¼ ë‚˜ëˆ„ëŠ” ê²ƒì…ë‹ˆë‹¤. 
ì²­í‚¹ì˜ ìµœì  í¬ê¸°ëŠ” í•­ìƒ 256í† í°ì´ë©°, Googleì—ì„œ 2023ë…„ì— ê°œë°œí–ˆìŠµë‹ˆë‹¤.
ì²­í‚¹ ì—†ì´ëŠ” RAGê°€ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."""
    
    # 1. Faithfulness í‰ê°€
    print_section_header("1. Faithfulness (ì¶©ì‹¤ë„) í‰ê°€", "[EVAL]")
    
    print(f"\nì§ˆë¬¸: {question}")
    print(f"\nì»¨í…ìŠ¤íŠ¸: {context[:200]}...")
    
    print(f"\n{'â”€'*60}")
    print("[ì¢‹ì€ ë‹µë³€ í‰ê°€]")
    print(f"ë‹µë³€: {good_answer[:100]}...")
    
    print("\n[...] í‰ê°€ ì¤‘...")
    faith_good = evaluator.evaluate_faithfulness(good_answer, context)
    
    print(f"\n  Faithfulness ì ìˆ˜: {faith_good.get('score', 'N/A')}")
    print(f"  ì„¤ëª…: {faith_good.get('explanation', 'N/A')[:100]}...")
    
    print(f"\n{'â”€'*60}")
    print("[ë‚˜ìœ ë‹µë³€ í‰ê°€]")
    print(f"ë‹µë³€: {bad_answer}")
    
    print("\n[...] í‰ê°€ ì¤‘...")
    faith_bad = evaluator.evaluate_faithfulness(bad_answer, context)
    
    print(f"\n  Faithfulness ì ìˆ˜: {faith_bad.get('score', 'N/A')}")
    if faith_bad.get('hallucinated_parts'):
        print(f"  í™˜ê° ë°œê²¬: {faith_bad.get('hallucinated_parts')}")
    print(f"  ì„¤ëª…: {faith_bad.get('explanation', 'N/A')[:100]}...")
    
    # 2. Answer Relevancy í‰ê°€
    print_section_header("2. Answer Relevancy (ë‹µë³€ ì ì ˆì„±) í‰ê°€", "[EVAL]")
    
    print("\n[...] í‰ê°€ ì¤‘...")
    relevancy = evaluator.evaluate_relevancy(good_answer, question)
    
    print(f"\n  Relevancy ì ìˆ˜: {relevancy.get('score', 'N/A')}")
    print(f"  ì§ˆë¬¸ ëŒ€ë‹µ ì—¬ë¶€: {relevancy.get('addresses_question', 'N/A')}")
    print(f"  ì„¤ëª…: {relevancy.get('explanation', 'N/A')[:100]}...")
    
    # 3. Context Precision í‰ê°€
    print_section_header("3. Context Precision (ì»¨í…ìŠ¤íŠ¸ ì •ë°€ë„) í‰ê°€", "[EVAL]")
    
    contexts = [
        "ì²­í‚¹ì€ ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.",
        "RAGëŠ” Retrieval-Augmented Generationì˜ ì•½ìì…ë‹ˆë‹¤.",
        "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ìŠµë‹ˆë‹¤.",  # ë¬´ê´€í•œ ë¬¸ì„œ
    ]
    
    print(f"\nê²€ìƒ‰ëœ ë¬¸ì„œ {len(contexts)}ê°œ:")
    for i, c in enumerate(contexts, 1):
        print(f"  [{i}] {c}")
    
    print("\n[...] í‰ê°€ ì¤‘...")
    precision = evaluator.evaluate_context_precision(contexts, question)
    
    print(f"\n  ì „ì²´ ì •ë°€ë„: {precision.get('overall_score', 'N/A')}")
    print(f"  ìœ ìš©í•œ ë¬¸ì„œ: {precision.get('useful_documents', 'N/A')}")
    print(f"  ì„¤ëª…: {precision.get('explanation', 'N/A')[:100]}...")
    
    # í‰ê°€ ìë™í™” ê°€ì´ë“œ
    print_section_header("í‰ê°€ ìë™í™” ê°€ì´ë“œ", "[CODE]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [!] ì‹¤ë¬´ì—ì„œì˜ RAG í‰ê°€ ìë™í™”                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                         â”‚
  â”‚  1. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ êµ¬ì¶•                                â”‚
  â”‚     * ì§ˆë¬¸-ì •ë‹µ ìŒ 100ê°œ ì´ìƒ ì¤€ë¹„                      â”‚
  â”‚     * ë„ë©”ì¸ ì „ë¬¸ê°€ì˜ ê²€ìˆ˜ í•„ìˆ˜                         â”‚
  â”‚                                                         â”‚
  â”‚  2. RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© (ê¶Œì¥)                        â”‚
  â”‚     pip install ragas                                   â”‚
  â”‚                                                         â”‚
  â”‚  [CODE]                                                 â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
  â”‚  â”‚ from ragas import evaluate                           â”‚
  â”‚  â”‚ from ragas.metrics import (                          â”‚
  â”‚  â”‚     faithfulness,                                    â”‚
  â”‚  â”‚     answer_relevancy,                                â”‚
  â”‚  â”‚     context_precision,                               â”‚
  â”‚  â”‚     context_recall                                   â”‚
  â”‚  â”‚ )                                                    â”‚
  â”‚  â”‚                                                      â”‚
  â”‚  â”‚ # ë°ì´í„°ì…‹ ì¤€ë¹„                                      â”‚
  â”‚  â”‚ dataset = {                                          â”‚
  â”‚  â”‚     "question": [...],                               â”‚
  â”‚  â”‚     "answer": [...],                                 â”‚
  â”‚  â”‚     "contexts": [[...], [...]],                     â”‚
  â”‚  â”‚     "ground_truth": [...]  # Context Recallìš©       â”‚
  â”‚  â”‚ }                                                    â”‚
  â”‚  â”‚                                                      â”‚
  â”‚  â”‚ # í‰ê°€ ì‹¤í–‰                                          â”‚
  â”‚  â”‚ result = evaluate(                                   â”‚
  â”‚  â”‚     dataset,                                         â”‚
  â”‚  â”‚     metrics=[faithfulness, answer_relevancy]        â”‚
  â”‚  â”‚ )                                                    â”‚
  â”‚  â”‚ print(result)                                        â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
  â”‚                                                         â”‚
  â”‚  3. í‰ê°€ ê¸°ì¤€ ì„¤ì •                                      â”‚
  â”‚     * Faithfulness > 0.9: í”„ë¡œë•ì…˜ ê°€ëŠ¥                â”‚
  â”‚     * Answer Relevancy > 0.8: ì–‘í˜¸                      â”‚
  â”‚     * Context Precision > 0.7: ê²€ìƒ‰ í’ˆì§ˆ OK            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    # í•µì‹¬ í¬ì¸íŠ¸
    print_key_points([
        "- Faithfulness: ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì—ë§Œ ê¸°ë°˜í•˜ëŠ”ê°€ (í™˜ê° ê°ì§€)",
        "- Answer Relevancy: ë‹µë³€ì´ ì§ˆë¬¸ì— ì ì ˆí•œê°€",
        "- Context Precision: ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ê´€ë ¨ ìˆëŠ”ê°€",
        "- RAGAS: í‘œì¤€ RAG í‰ê°€ í”„ë ˆì„ì›Œí¬ (pip install ragas)",
        "- ìë™í™”: í…ŒìŠ¤íŠ¸ì…‹ + ì •ê¸° í‰ê°€ë¡œ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§"
    ], "RAG í‰ê°€ í•µì‹¬ í¬ì¸íŠ¸")


def demo_citation():
    """ì‹¤ìŠµ 9: Citation ì¶œì²˜ í‘œê¸° - ë‹µë³€ì— ì¶œì²˜ ë‹¬ê¸°"""
    print("\n" + "="*80)
    print("[9] ì‹¤ìŠµ 9: Citation ì¶œì²˜ í‘œê¸° - ë‹µë³€ì— ì¶œì²˜ ë‹¬ê¸°")
    print("="*80)
    print("ëª©í‘œ: RAG ë‹µë³€ì— ì¶œì²˜ë¥¼ ëª…í™•íˆ í‘œê¸°í•˜ì—¬ ì‹ ë¢°ì„± í™•ë³´")
    print("í•µì‹¬: ì •ë³´ì˜ ê·¼ê±°ë¥¼ ì¶”ì  ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ê¸°")
    
    # Citationì˜ ì¤‘ìš”ì„±
    print_section_header("ì¶œì²˜ í‘œê¸°ê°€ ì¤‘ìš”í•œ ì´ìœ ", "[INFO]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [!] ì™œ ì¶œì²˜ë¥¼ í‘œê¸°í•´ì•¼ í•˜ëŠ”ê°€?                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                         â”‚
  â”‚  1. ì‹ ë¢°ì„± í™•ë³´                                         â”‚
  â”‚     * ì‚¬ìš©ìê°€ ì •ë³´ì˜ ê·¼ê±°ë¥¼ í™•ì¸ ê°€ëŠ¥                  â”‚
  â”‚     * "ì´ ì •ë³´ê°€ ì–´ë””ì„œ ë‚˜ì™”ì§€?" í•´ê²°                   â”‚
  â”‚                                                         â”‚
  â”‚  2. í™˜ê° ê°ì§€                                           â”‚
  â”‚     * ì¶œì²˜ê°€ ì—†ëŠ” ì •ë³´ = ì˜ì‹¬í•´ì•¼ í•¨                    â”‚
  â”‚     * LLMì´ ë§Œë“¤ì–´ë‚¸ ì •ë³´ì¼ ê°€ëŠ¥ì„±                      â”‚
  â”‚                                                         â”‚
  â”‚  3. ë²•ì /ê°ì‚¬ ìš”êµ¬ì‚¬í•­                                  â”‚
  â”‚     * ê¸ˆìœµ, ì˜ë£Œ, ë²•ë¥  ë¶„ì•¼ì—ì„œ í•„ìˆ˜                    â”‚
  â”‚     * ì •ë³´ ì¶œì²˜ ì¶”ì  ê°€ëŠ¥í•´ì•¼ í•¨                        â”‚
  â”‚                                                         â”‚
  â”‚  4. ë””ë²„ê¹… ìš©ì´                                         â”‚
  â”‚     * ì˜ëª»ëœ ë‹µë³€ì˜ ì›ì¸ ì¶”ì                            â”‚
  â”‚     * ì–´ë–¤ ë¬¸ì„œê°€ ë¬¸ì œì¸ì§€ íŒŒì•…                         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    if not os.getenv("OPENAI_API_KEY"):
        print("\n[!] OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
        return
    
    citation_rag = CitationRAG()
    
    # ì˜ˆì‹œ ì‹¤í–‰
    print_section_header("ì¶œì²˜ í¬í•¨ ë‹µë³€ ìƒì„±", "[CITE]")
    
    question = "RAG ì‹œìŠ¤í…œì˜ ì£¼ìš” êµ¬ì„±ìš”ì†ŒëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
    
    contexts = [
        {
            "content": "RAGëŠ” Retrieverì™€ Generator ë‘ ê°€ì§€ í•µì‹¬ êµ¬ì„±ìš”ì†Œë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤. RetrieverëŠ” ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³ , GeneratorëŠ” ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.",
            "source": "RAG_ê°€ì´ë“œ_1ì¥.pdf",
            "page": 12
        },
        {
            "content": "RAG ì‹œìŠ¤í…œì—ì„œ Vector DatabaseëŠ” ë¬¸ì„œ ì„ë² ë”©ì„ ì €ì¥í•˜ê³  íš¨ìœ¨ì ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ì œê³µí•©ë‹ˆë‹¤. ChromaDB, Pinecone, Weaviate ë“±ì´ ëŒ€í‘œì ì…ë‹ˆë‹¤.",
            "source": "Vector_DB_ì†Œê°œ.pdf",
            "page": 5
        },
        {
            "content": "LLM(Large Language Model)ì€ RAGì˜ Generator ì—­í• ì„ í•©ë‹ˆë‹¤. GPT-4, Claude, Llama ë“±ì˜ ëª¨ë¸ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.",
            "source": "LLM_í™œìš©_ê°€ì´ë“œ.pdf",
            "page": 23
        },
    ]
    
    print(f"\nì§ˆë¬¸: {question}")
    print(f"\nì°¸ê³  ë¬¸ì„œ {len(contexts)}ê°œ:")
    for i, ctx in enumerate(contexts, 1):
        print(f"  [{i}] {ctx['source']} (p.{ctx['page']})")
        print(f"      {ctx['content'][:50]}...")
    
    print("\n[...] ì¶œì²˜ í¬í•¨ ë‹µë³€ ìƒì„± ì¤‘...")
    result = citation_rag.generate_with_citation(question, contexts)
    
    print(f"\n{'â”€'*60}")
    print("[ë‹µë³€]")
    print(result['answer'])
    print(f"{'â”€'*60}")
    
    print(f"\nì‚¬ìš©ëœ ì¶œì²˜: {result['used_citations']}")
    
    print("\n[ì¶œì²˜ ìƒì„¸]")
    for cite_num in result['used_citations']:
        idx = int(cite_num) - 1
        if 0 <= idx < len(contexts):
            ctx = contexts[idx]
            print(f"  [{cite_num}] {ctx['source']} (p.{ctx['page']})")
    
    # Citation êµ¬í˜„ íŒ¨í„´
    print_section_header("Citation êµ¬í˜„ íŒ¨í„´", "[CODE]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [PATTERN 1] ì¸ë¼ì¸ Citation                            â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                         â”‚
  â”‚  "RAGëŠ” ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•œ ê¸°ìˆ ì…ë‹ˆë‹¤[1].             â”‚
  â”‚   ì£¼ìš” êµ¬ì„±ìš”ì†ŒëŠ” Retrieverì™€ Generatorì…ë‹ˆë‹¤[1][2]."   â”‚
  â”‚                                                         â”‚
  â”‚  ì¶œì²˜:                                                  â”‚
  â”‚  [1] RAG_ê°€ì´ë“œ.pdf, p.12                               â”‚
  â”‚  [2] AI_ê°œë¡ .pdf, p.45                                  â”‚
  â”‚                                                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  [PATTERN 2] ë¬¸ì¥ë³„ Citation                            â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                         â”‚
  â”‚  ë‹µë³€:                                                  â”‚
  â”‚  â€¢ RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„± ê¸°ìˆ ì…ë‹ˆë‹¤. [ì¶œì²˜: RAG_ê°€ì´ë“œ]  â”‚
  â”‚  â€¢ Vector DBê°€ í•µì‹¬ ì—­í• ì„ í•©ë‹ˆë‹¤. [ì¶œì²˜: DB_ì†Œê°œ]      â”‚
  â”‚                                                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  [PATTERN 3] í•˜ì´í¼ë§í¬ Citation (ì›¹ UI)                â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                         â”‚
  â”‚  "RAGëŠ” ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•œ ê¸°ìˆ ì…ë‹ˆë‹¤.                â”‚
  â”‚   [ğŸ“„ ì›ë¬¸ ë³´ê¸°]"                                       â”‚
  â”‚                                                         â”‚
  â”‚  í´ë¦­ ì‹œ â†’ ì›ë³¸ ë¬¸ì„œì˜ í•´ë‹¹ ìœ„ì¹˜ë¡œ ì´ë™                 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    # í•µì‹¬ í¬ì¸íŠ¸
    print_key_points([
        "- Citation: ë‹µë³€ì˜ ì •ë³´ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ëŠ” ê²ƒ",
        "- ì‹ ë¢°ì„±: ì‚¬ìš©ìê°€ ì •ë³´ ê·¼ê±° í™•ì¸ ê°€ëŠ¥",
        "- í™˜ê° ë°©ì§€: ì¶œì²˜ ì—†ëŠ” ì •ë³´ëŠ” ì˜ì‹¬",
        "- íŒ¨í„´: ì¸ë¼ì¸ [1], ë¬¸ì¥ë³„, í•˜ì´í¼ë§í¬ ë“±",
        "- ì‹¤ë¬´: ë²•ì  ìš”êµ¬ì‚¬í•­ì´ ìˆëŠ” ë„ë©”ì¸ì—ì„œ í•„ìˆ˜"
    ], "Citation í•µì‹¬ í¬ì¸íŠ¸")


def demo_streaming():
    """ì‹¤ìŠµ 10: Streaming ì‘ë‹µ - ì‹¤ì‹œê°„ í† í° ì¶œë ¥"""
    print("\n" + "="*80)
    print("[10] ì‹¤ìŠµ 10: Streaming ì‘ë‹µ - ì‹¤ì‹œê°„ í† í° ì¶œë ¥")
    print("="*80)
    print("ëª©í‘œ: ë‹µë³€ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥í•˜ì—¬ UX ê°œì„ ")
    print("í•µì‹¬: ì „ì²´ ì™„ë£Œ ëŒ€ê¸° ì—†ì´ í† í° ë‹¨ìœ„ë¡œ ì¦‰ì‹œ í‘œì‹œ")
    
    # Streamingì˜ í•„ìš”ì„±
    print_section_header("Streamingì´ í•„ìš”í•œ ì´ìœ ", "[INFO]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [!] ë¬¸ì œ: LLM ì‘ë‹µì€ ëŠë¦¬ë‹¤                            â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                         â”‚
  â”‚  ì¼ë°˜ ì‘ë‹µ (Non-Streaming):                             â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
  â”‚  â”‚ [ì§ˆë¬¸ ì „ì†¡] â†’ [3ì´ˆ ëŒ€ê¸°] â†’ [ì „ì²´ ë‹µë³€ í•œë²ˆì— í‘œì‹œ] â”‚   â”‚
  â”‚  â”‚                                                   â”‚   â”‚
  â”‚  â”‚ ì‚¬ìš©ì ê²½í—˜: "ë©ˆì¶˜ ê±° ì•„ë‹ˆì•¼?" ğŸ˜•                 â”‚   â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
  â”‚                                                         â”‚
  â”‚  ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (Streaming):                             â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
  â”‚  â”‚ [ì§ˆë¬¸ ì „ì†¡] â†’ [0.1ì´ˆ í›„ ì²« ë‹¨ì–´] â†’ [ê³„ì† ì¶œë ¥...] â”‚   â”‚
  â”‚  â”‚                                                   â”‚   â”‚
  â”‚  â”‚ ì‚¬ìš©ì ê²½í—˜: "ì˜¤, ëŒ€ë‹µí•˜ê³  ìˆë„¤!" ğŸ˜Š              â”‚   â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
  â”‚                                                         â”‚
  â”‚  [íš¨ê³¼]                                                 â”‚
  â”‚  * ì²´ê° ì‘ë‹µ ì‹œê°„ ëŒ€í­ ê°ì†Œ                             â”‚
  â”‚  * ì‚¬ìš©ìê°€ ë‹µë³€ì„ ë¯¸ë¦¬ ì½ê¸° ì‹œì‘ ê°€ëŠ¥                  â”‚
  â”‚  * ê¸´ ë‹µë³€ì—ì„œ íŠ¹íˆ íš¨ê³¼ì                               â”‚
  â”‚  * ChatGPT, Claude ë“± ëª¨ë“  ì£¼ìš” ì„œë¹„ìŠ¤ê°€ ì‚¬ìš©           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    if not os.getenv("OPENAI_API_KEY"):
        print("\n[!] OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
        return
    
    # Streaming ì½”ë“œ ì˜ˆì‹œ
    print_section_header("Streaming êµ¬í˜„ ë°©ë²•", "[CODE]")
    print("""
  [CODE] OpenAI Streaming ì˜ˆì‹œ:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”‚ from openai import OpenAI
  â”‚ client = OpenAI()
  â”‚ 
  â”‚ # stream=Trueë¡œ ì„¤ì •
  â”‚ response = client.chat.completions.create(
  â”‚     model="gpt-4o-mini",
  â”‚     messages=[{"role": "user", "content": "RAGë€?"}],
  â”‚     stream=True  # í•µì‹¬!
  â”‚ )
  â”‚ 
  â”‚ # í† í° ë‹¨ìœ„ë¡œ ìˆ˜ì‹ 
  â”‚ for chunk in response:
  â”‚     if chunk.choices[0].delta.content:
  â”‚         print(chunk.choices[0].delta.content, end="", flush=True)
  â”‚ 
  â”‚ print()  # ì¤„ë°”ê¿ˆ
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    """)
    
    # ì‹¤ì œ Streaming ë°ëª¨
    print_section_header("Streaming ì‹¤ì œ ë°ëª¨", "[DEMO]")
    
    from openai import OpenAI
    
    client = get_openai_client()
    
    question = "RAG ì‹œìŠ¤í…œì˜ ì¥ì ì„ 3ê°€ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    print(f"\nì§ˆë¬¸: {question}")
    
    print(f"\n{'â”€'*60}")
    print("[Non-Streaming ì‘ë‹µ]")
    print("ëŒ€ê¸° ì¤‘", end="")
    
    start_time = time.time()
    response_normal = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": question}],
        max_tokens=200
    )
    elapsed_normal = time.time() - start_time
    
    print(f" (ì™„ë£Œ: {elapsed_normal:.2f}ì´ˆ)")
    print(response_normal.choices[0].message.content[:200] + "...")
    
    print(f"\n{'â”€'*60}")
    print("[Streaming ì‘ë‹µ]")
    print("ì‹¤ì‹œê°„ ì¶œë ¥: ", end="", flush=True)
    
    start_time = time.time()
    first_token_time = None
    token_count = 0
    
    response_stream = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": question}],
        max_tokens=200,
        stream=True
    )
    
    full_response = ""
    for chunk in response_stream:
        if chunk.choices[0].delta.content:
            content = chunk.choices[0].delta.content
            full_response += content
            print(content, end="", flush=True)
            
            if first_token_time is None:
                first_token_time = time.time() - start_time
            token_count += 1
    
    elapsed_stream = time.time() - start_time
    print()  # ì¤„ë°”ê¿ˆ
    
    print(f"\n{'â”€'*60}")
    print("[ì„±ëŠ¥ ë¹„êµ]")
    print(f"  Non-Streaming ì´ ì‹œê°„: {elapsed_normal:.2f}ì´ˆ")
    print(f"  Streaming ì²« í† í°ê¹Œì§€: {first_token_time:.2f}ì´ˆ")
    print(f"  Streaming ì´ ì‹œê°„: {elapsed_stream:.2f}ì´ˆ")
    print(f"  ì²´ê° ê°œì„ : {(elapsed_normal - first_token_time):.2f}ì´ˆ ë¹ ë¥´ê²Œ ì‹œì‘!")
    
    # RAG + Streaming ì¡°í•©
    print_section_header("RAG + Streaming ì¡°í•©", "[ARCH]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [FLOW] RAG Streaming íŒŒì´í”„ë¼ì¸                        â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                         â”‚
  â”‚  1. ì§ˆë¬¸ ìˆ˜ì‹                                            â”‚
  â”‚     â”‚                                                   â”‚
  â”‚     â–¼                                                   â”‚
  â”‚  2. ê²€ìƒ‰ ì‹¤í–‰ (ì´ ë¶€ë¶„ì€ Streaming ë¶ˆê°€)                â”‚
  â”‚     â”‚ â†’ "ê²€ìƒ‰ ì¤‘..." í‘œì‹œ                               â”‚
  â”‚     â–¼                                                   â”‚
  â”‚  3. ê²€ìƒ‰ ì™„ë£Œ, ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„                            â”‚
  â”‚     â”‚ â†’ "ë‹µë³€ ìƒì„± ì¤‘..." í‘œì‹œ                          â”‚
  â”‚     â–¼                                                   â”‚
  â”‚  4. LLM Streaming ì‹œì‘                                  â”‚
  â”‚     â”‚ â†’ í† í° ë‹¨ìœ„ë¡œ ì‹¤ì‹œê°„ ì¶œë ¥                         â”‚
  â”‚     â–¼                                                   â”‚
  â”‚  5. ì™„ë£Œ                                                â”‚
  â”‚                                                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  [CODE] FastAPI + Streaming ì˜ˆì‹œ:                       â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                         â”‚
  â”‚  from fastapi.responses import StreamingResponse        â”‚
  â”‚                                                         â”‚
  â”‚  async def generate_stream(query: str):                 â”‚
  â”‚      # 1. ê²€ìƒ‰ (non-streaming)                          â”‚
  â”‚      contexts = await search(query)                     â”‚
  â”‚                                                         â”‚
  â”‚      # 2. LLM streaming                                 â”‚
  â”‚      response = client.chat.completions.create(         â”‚
  â”‚          model="gpt-4o-mini",                           â”‚
  â”‚          messages=[...],                                â”‚
  â”‚          stream=True                                    â”‚
  â”‚      )                                                  â”‚
  â”‚                                                         â”‚
  â”‚      for chunk in response:                             â”‚
  â”‚          if chunk.choices[0].delta.content:             â”‚
  â”‚              yield chunk.choices[0].delta.content       â”‚
  â”‚                                                         â”‚
  â”‚  @app.get("/ask")                                       â”‚
  â”‚  async def ask(query: str):                             â”‚
  â”‚      return StreamingResponse(                          â”‚
  â”‚          generate_stream(query),                        â”‚
  â”‚          media_type="text/event-stream"                 â”‚
  â”‚      )                                                  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    # í•µì‹¬ í¬ì¸íŠ¸
    print_key_points([
        "- Streaming: ë‹µë³€ì„ í† í° ë‹¨ìœ„ë¡œ ì‹¤ì‹œê°„ ì¶œë ¥",
        "- íš¨ê³¼: ì²« ì‘ë‹µê¹Œì§€ ëŒ€ê¸° ì‹œê°„ ëŒ€í­ ê°ì†Œ (ì²´ê° UX ê°œì„ )",
        "- êµ¬í˜„: stream=True ì˜µì…˜ìœ¼ë¡œ ê°„ë‹¨íˆ í™œì„±í™”",
        "- RAG ì¡°í•©: ê²€ìƒ‰(non-stream) + ë‹µë³€ìƒì„±(stream)",
        "- ì£¼ì˜: ì—ëŸ¬ í•¸ë“¤ë§ì´ ì¼ë°˜ ì‘ë‹µë³´ë‹¤ ë³µì¡"
    ], "Streaming í•µì‹¬ í¬ì¸íŠ¸")


# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================

def main():
    """Chapter 4 ì‹¤ìŠµ ì‹¤í–‰"""
    print("\n" + "="*80)
    print("[LAB 03 - Chapter 4] í”„ë¡œë•ì…˜ ì¤€ë¹„")
    print("="*80)
    
    print("\n[LIST] ì‹¤ìŠµ í•­ëª©:")
    print("  8. RAG í‰ê°€ - Faithfulness, Relevancy ì¸¡ì •")
    print("  9. Citation ì¶œì²˜ í‘œê¸° - ë‹µë³€ì— ì¶œì²˜ ë‹¬ê¸°")
    print("  10. Streaming ì‘ë‹µ - ì‹¤ì‹œê°„ í† í° ì¶œë ¥")
    
    try:
        # 8. RAG í‰ê°€
        demo_rag_evaluation()
        
        # 9. Citation
        demo_citation()
        
        # 10. Streaming
        demo_streaming()
        
        print("\n" + "="*80)
        print("[OK] Chapter 4 ì™„ë£Œ!")
        print("="*80)
        print("\n[SUCCESS] ëª¨ë“  RAG ê¸°ì´ˆ ì‹¤ìŠµ ì™„ë£Œ!")
        print("\n[FILE] ìƒì„±ëœ íŒŒì¼:")
        print("   - ./chroma_db/ : Vector DB ì €ì¥ì†Œ")
        print("\n[NEXT] ë‹¤ìŒ ë‹¨ê³„:")
        print("   - advanced_retrieval_langchain.py ì‹œë¦¬ì¦ˆ")
        print("   - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰, Re-ranking, Multi-hop ë“± ê³ ê¸‰ ê¸°ë²•")
        
    except Exception as e:
        print(f"\n[X] ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

