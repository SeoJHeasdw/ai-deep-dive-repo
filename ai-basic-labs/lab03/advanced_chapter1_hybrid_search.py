"""
Advanced RAG - Chapter 1: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
Sparse (BM25) + Dense (Vector) ê²€ìƒ‰ ê²°í•©

ì‹¤ìŠµ í•­ëª©:
1. Sparse + Dense í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
   - BM25 (í‚¤ì›Œë“œ ë§¤ì¹­)
   - Vector DB (ì˜ë¯¸ì  ìœ ì‚¬ë„)
   - ê°€ì¤‘ ê²°í•© (alpha íŒŒë¼ë¯¸í„°)

í•™ìŠµ ëª©í‘œ:
- Sparseì™€ Dense ê²€ìƒ‰ì˜ ì¥ë‹¨ì  ì´í•´
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì˜ ì›ë¦¬ íŒŒì•…
- í•œê¸€ í† í°í™” ë¬¸ì œ ì¸ì‹
- ê²€ìƒ‰ ë°©ë²•ë³„ ê²°ê³¼ ë¹„êµ
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

# LangChain
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

# Sparse ê²€ìƒ‰ (BM25)
from rank_bm25 import BM25Okapi

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import chromadb
from dotenv import load_dotenv
import numpy as np
import pdfplumber

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ .env íŒŒì¼ ë¡œë“œ
project_root = Path(__file__).parent.parent
load_dotenv(dotenv_path=project_root / '.env')

# ê³µí†µ ìœ í‹¸ë¦¬í‹° importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(project_root))

# ê³µí†µ ë°ì´í„° ì„í¬íŠ¸
from shared_data import SAMPLE_TEXT, MIN_TEXT_LENGTH, get_sample_or_document_text


@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    content: str
    score: float
    metadata: Dict[str, Any]
    rank: int


class DocumentProcessor:
    """ë¬¸ì„œ ì²˜ë¦¬ ë° ì²­í‚¹"""
    
    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def load_pdf(self, file_path: str) -> str:
        """PDF íŒŒì¼ ë¡œë“œ"""
        text = ""
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        return text
    
    def chunk_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        chunks = []
        start = 0
        text_length = len(text)
        
        while start < text_length:
            end = start + self.chunk_size
            
            if end >= text_length:
                chunk = text[start:].strip()
                if chunk:
                    chunks.append(chunk)
                break
            
            # ë¬¸ì¥ ê²½ê³„ ì°¾ê¸°
            best_end = -1
            
            # ë‹¨ë½ ë
            double_newline = text.rfind('\n\n', start, end + 50)
            if double_newline != -1:
                best_end = double_newline + 2
            
            # ë¬¸ì¥ ë
            if best_end == -1:
                for i in range(end, max(start, end - 100), -1):
                    if i < text_length - 1 and text[i] == '.' and text[i+1] == '\n':
                        best_end = i + 2
                        break
            
            # ë§ˆì¹¨í‘œ + ê³µë°±
            if best_end == -1:
                period_space = text.rfind('. ', start, end + 30)
                if period_space != -1:
                    best_end = period_space + 2
            
            # ì¤„ë°”ê¿ˆ
            if best_end == -1:
                newline = text.rfind('\n', start, end + 20)
                if newline != -1:
                    best_end = newline + 1
            
            # ê³µë°±
            if best_end == -1:
                space = text.rfind(' ', start, end)
                if space != -1 and space > start + self.chunk_size // 2:
                    best_end = space + 1
            
            # ê°•ì œë¡œ ìë¥´ê¸°
            if best_end == -1:
                best_end = end
            
            chunk = text[start:best_end].strip()
            if chunk:
                chunks.append(chunk)
            
            next_start = best_end - self.chunk_overlap
            if next_start <= start:
                next_start = best_end
            
            start = next_start
        
        return chunks
    
    def create_chunks(self, text: str, metadata: Optional[Dict] = None) -> List[Document]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ Document ê°ì²´ ìƒì„±"""
        chunks = self.chunk_text(text)
        
        documents = []
        for i, chunk in enumerate(chunks):
            doc_metadata = metadata.copy() if metadata else {}
            doc_metadata.update({
                "chunk_id": i,
                "chunk_size": len(chunk),
                "total_chunks": len(chunks)
            })
            documents.append(Document(page_content=chunk, metadata=doc_metadata))
        
        return documents


class HybridRetriever:
    """Sparse (BM25) + Dense (Vector) í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
    
    # í•œê¸€ ì¡°ì‚¬ íŒ¨í„´ (ê°„ë‹¨ ë²„ì „)
    KOREAN_PARTICLES = [
        'ì´ë€', 'ì´ë€?', 'ë€', 'ë€?', 'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼',
        'ì˜', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ê¹Œì§€',
        'ë¶€í„°', 'ì´ë‹¤', 'ì…ë‹ˆë‹¤', 'ì¸ê°€', 'ì¸ê°€?', 'ì¸ì§€', 'í•˜ëŠ”', 'ë˜ëŠ”'
    ]
    
    @staticmethod
    def tokenize_korean(text: str) -> List[str]:
        """
        ê°„ë‹¨í•œ í•œê¸€ í† í°í™” (êµìœ¡ìš©)
        
        âš ï¸ í•œê³„:
        - ì‹¤ì œ í˜•íƒœì†Œ ë¶„ì„ì´ ì•„ë‹˜ (ë‹¨ìˆœ ê·œì¹™ ê¸°ë°˜ ì¡°ì‚¬ ì œê±°)
        - ë³µì¡í•œ ì–´ë¯¸ ì²˜ë¦¬ ë¶ˆê°€
        
        ğŸ”§ ì‹¤ë¬´ ê¶Œì¥:
        - KoNLPy (Mecab, Okt, Komoran ë“±) í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚¬ìš©
        """
        import re
        
        # êµ¬ë‘ì ì„ ê³µë°±ìœ¼ë¡œ
        text = re.sub(r'[.,!?;:()"\'\[\]{}]', ' ', text)
        
        # ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
        tokens = text.split()
        
        # ì¡°ì‚¬ ì œê±° ì‹œë„
        cleaned_tokens = []
        for token in tokens:
            cleaned = token
            for particle in sorted(HybridRetriever.KOREAN_PARTICLES, key=len, reverse=True):
                if cleaned.endswith(particle) and len(cleaned) > len(particle):
                    cleaned = cleaned[:-len(particle)]
                    break
            if cleaned:
                cleaned_tokens.append(cleaned)
        
        return cleaned_tokens
    
    def __init__(
        self,
        documents: List[Document],
        embeddings: OpenAIEmbeddings,
        persist_directory: str = "./chroma_db",
        collection_name: str = "hybrid_search"
    ):
        self.documents = documents
        self.embeddings = embeddings
        
        # Dense ê²€ìƒ‰: Vector DB (Chroma)
        print(f"Dense ê²€ìƒ‰ ì¤€ë¹„ ì¤‘... (ì»¬ë ‰ì…˜: {collection_name})")
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
        try:
            chroma_client = chromadb.PersistentClient(path=persist_directory)
            chroma_client.delete_collection(name=collection_name)
        except:
            pass
        
        self.vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=embeddings,
            persist_directory=persist_directory,
            collection_name=collection_name
        )
        
        # Sparse ê²€ìƒ‰: BM25
        print("Sparse ê²€ìƒ‰ ì¤€ë¹„ ì¤‘... (BM25 + í•œê¸€ í† í°í™”)")
        self.corpus = [doc.page_content for doc in documents]
        self.tokenized_corpus = [self.tokenize_korean(doc) for doc in self.corpus]
        self.bm25 = BM25Okapi(self.tokenized_corpus)
        
        print(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì¤€ë¹„ ì™„ë£Œ (ë¬¸ì„œ ìˆ˜: {len(documents)})")
    
    def sparse_search(self, query: str, k: int = 10) -> List[SearchResult]:
        """BM25 Sparse ê²€ìƒ‰"""
        tokenized_query = self.tokenize_korean(query)
        scores = self.bm25.get_scores(tokenized_query)
        
        k = min(k, len(self.documents))
        top_indices = np.argsort(scores)[::-1]
        
        results = []
        rank = 1
        for idx in top_indices:
            score = float(scores[idx])
            
            if score == 0:
                continue
            
            results.append(SearchResult(
                content=self.documents[idx].page_content,
                score=score,
                metadata={**self.documents[idx].metadata, "matched_tokens": tokenized_query},
                rank=rank
            ))
            rank += 1
            
            if len(results) >= k:
                break
        
        if not results:
            results.append(SearchResult(
                content=f"[í‚¤ì›Œë“œ '{' '.join(tokenized_query)}' ë§¤ì¹­ ì—†ìŒ]",
                score=0.0,
                metadata={"no_match": True},
                rank=1
            ))
        
        return results
    
    def dense_search(self, query: str, k: int = 10) -> List[SearchResult]:
        """Vector DB Dense ê²€ìƒ‰"""
        k = min(k, len(self.documents))
        docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=k)
        
        results = []
        for rank, (doc, score) in enumerate(docs_with_scores):
            # L2 ê±°ë¦¬ë¥¼ 0~1 ì ìˆ˜ë¡œ ë³€í™˜
            distance_score = 1 / (1 + score)
            results.append(SearchResult(
                content=doc.page_content,
                score=float(distance_score),
                metadata={**doc.metadata, "raw_distance": float(score)},
                rank=rank + 1
            ))
        
        return results
    
    def hybrid_search(
        self,
        query: str,
        k: int = 10,
        alpha: float = 0.5
    ) -> List[SearchResult]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Sparse + Dense)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            alpha: Dense ê°€ì¤‘ì¹˜ (0~1, 1-alphaê°€ Sparse ê°€ì¤‘ì¹˜)
        """
        # ë‘ ê²€ìƒ‰ ì‹¤í–‰
        sparse_results = self.sparse_search(query, k=k*2)
        dense_results = self.dense_search(query, k=k*2)
        
        # ì •ê·œí™” (0~1 ë²”ìœ„ë¡œ)
        def normalize_scores(results: List[SearchResult]) -> List[SearchResult]:
            if not results or results[0].metadata.get("no_match"):
                return results
            
            scores = [r.score for r in results]
            min_score = min(scores)
            max_score = max(scores)
            
            if max_score == min_score:
                for r in results:
                    r.score = 1.0
                return results
            
            for r in results:
                r.score = (r.score - min_score) / (max_score - min_score)
            
            return results
        
        sparse_results = normalize_scores(sparse_results)
        dense_results = normalize_scores(dense_results)
        
        # ê²°í•©
        combined_scores = {}
        
        for result in sparse_results:
            if result.metadata.get("no_match"):
                continue
            content = result.content
            combined_scores[content] = {
                "sparse": result.score * (1 - alpha),
                "dense": 0.0,
                "metadata": result.metadata
            }
        
        for result in dense_results:
            content = result.content
            if content in combined_scores:
                combined_scores[content]["dense"] = result.score * alpha
            else:
                combined_scores[content] = {
                    "sparse": 0.0,
                    "dense": result.score * alpha,
                    "metadata": result.metadata
                }
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
        final_results = []
        for content, scores in combined_scores.items():
            final_score = scores["sparse"] + scores["dense"]
            final_results.append(SearchResult(
                content=content,
                score=final_score,
                metadata={
                    **scores["metadata"],
                    "sparse_score": scores["sparse"],
                    "dense_score": scores["dense"]
                },
                rank=0
            ))
        
        final_results.sort(key=lambda x: x.score, reverse=True)
        
        for rank, result in enumerate(final_results[:k], 1):
            result.rank = rank
        
        return final_results[:k]


def format_chunk(content: str, indent: str = "      ") -> str:
    """ì²­í¬ ë‚´ìš©ì„ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…"""
    lines = content.strip().split('\n')
    formatted_lines = []
    for line in lines:
        line = line.strip()
        if line:
            formatted_lines.append(f"{indent}{line}")
    return '\n'.join(formatted_lines)


def print_search_result(result: SearchResult, index: int, show_full: bool = False):
    """ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥"""
    print(f"\n  [{index}] ì ìˆ˜: {result.score:.4f} | ì²­í¬ #{result.metadata.get('chunk_id', -1) + 1}")
    
    if show_full:
        print(f"  {'â”€'*50}")
        lines = result.content.strip().split('\n')
        for line in lines[:10]:
            print(f"      {line}")
        if len(lines) > 10:
            print(f"      ... ({len(lines) - 10}ì¤„ ë” ìˆìŒ)")
        print(f"  {'â”€'*50}")
    else:
        preview = result.content.replace('\n', ' ')[:100]
        print(f"      {preview}...")


def experiment_hybrid_search(text: str = None):
    """ì‹¤ìŠµ 1: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
    print("\n" + "="*80)
    print("[1] ì‹¤ìŠµ 1: Sparse + Dense í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰")
    print("="*80)
    print("ëª©í‘œ: BM25(í‚¤ì›Œë“œ) + Vector(ì˜ë¯¸)ë¥¼ ê²°í•©í•˜ì—¬ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ")
    
    sample_text = text or SAMPLE_TEXT
    
    # ë¬¸ì„œ ì²˜ë¦¬
    print(f"\n[DOC] ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...")
    processor = DocumentProcessor(chunk_size=500, chunk_overlap=50)
    documents = processor.create_chunks(sample_text, metadata={"source": "AI_ê°€ì´ë“œ"})
    print(f"  - ìƒì„±ëœ ì²­í¬: {len(documents)}ê°œ")
    
    # ì„ë² ë”© ëª¨ë¸
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
    retriever = HybridRetriever(
        documents=documents,
        embeddings=embeddings,
        collection_name="hybrid_exp"
    )
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_query = "ë”¥ëŸ¬ë‹ì˜ ì£¼ìš” ì•„í‚¤í…ì²˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
    print(f"\n[*] ì¿¼ë¦¬: '{test_query}'")
    
    # 1. Sparse ê²€ìƒ‰ë§Œ
    print(f"\n{'â”€'*60}")
    print("[>] Sparse ê²€ìƒ‰ (BM25)")
    print(f"{'â”€'*60}")
    sparse_results = retriever.sparse_search(test_query, k=3)
    for i, result in enumerate(sparse_results, 1):
        print_search_result(result, i, show_full=(i==1))
    
    # 2. Dense ê²€ìƒ‰ë§Œ
    print(f"\n{'â”€'*60}")
    print("[>] Dense ê²€ìƒ‰ (Vector DB)")
    print(f"{'â”€'*60}")
    dense_results = retriever.dense_search(test_query, k=3)
    for i, result in enumerate(dense_results, 1):
        print_search_result(result, i, show_full=(i==1))
    
    # 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    print(f"\n{'â”€'*60}")
    print("[>] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (alpha=0.5)")
    print(f"{'â”€'*60}")
    hybrid_results = retriever.hybrid_search(test_query, k=3, alpha=0.5)
    for i, result in enumerate(hybrid_results, 1):
        print_search_result(result, i, show_full=(i==1))
    
    # í•µì‹¬ í¬ì¸íŠ¸
    print("\n" + "="*60)
    print("[TIP] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í•µì‹¬:")
    print("="*60)
    print("  - Sparse (BM25): í‚¤ì›Œë“œ ë§¤ì¹­ (ì •í™•í•œ ìš©ì–´ ê²€ìƒ‰ì— ê°•í•¨)")
    print("  - Dense (Vector): ì˜ë¯¸ì  ìœ ì‚¬ë„ (ë™ì˜ì–´, ìœ ì‚¬ í‘œí˜„ì— ê°•í•¨)")
    print("  - Hybrid: ë‘ ë°©ë²•ì˜ ì¥ì  ê²°í•©")
    print("  - alpha: Dense ê°€ì¤‘ì¹˜ (0.5 = 50:50)")


def main():
    """Chapter 1 ì‹¤í–‰"""
    print("\n" + "="*80)
    print("[Advanced RAG - Chapter 1] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰")
    print("="*80)
    
    print("\n[LIST] ì‹¤ìŠµ í•­ëª©:")
    print("  1. Sparse + Dense í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰")
    
    try:
        experiment_hybrid_search()
        
        print("\n" + "="*80)
        print("[OK] Chapter 1 ì™„ë£Œ!")
        print("="*80)
        print("\n[NEXT] ë‹¤ìŒ ë‹¨ê³„:")
        print("   - advanced_chapter2_reranking.py : Re-ranking (Cross-Encoder)")
        
    except Exception as e:
        print(f"\n[X] ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

